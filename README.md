# Опсиание проекта

Проект представляет собой алгоритм обучения с подкреплением PPO с интеграцией в него алгоритма RRT* для планирования оптимального пути и внедрением потенциальных полей для расчета наград и штрафов. 

### Actor-critic

Алгоритм PPO основан на архитектуре actor-critic, то есть содержит в себе две нейронные сети. 

1. Актор (actor):
  - На вход подается состояние состояние агента
  - Имеет два скрытых слоя по 128 нейронов с функцией активации ReLu
  - На выход дает вероятности выбора каждого действия (вектор)

2. Критик (critic): 
    - На вход подается состояние агента
    - Имеет два скрытых слоя по 128 нейронов с функцией активации ReLu
    - На выходе оценка ценности текущего состояния (скаляр)

### Алгоритм RRT*

Алгоритм RRT* предназначен для построения оптимального пути от начального положения дл цели. В данной ситуации путь строится на основе заранее известной grid_map, но в перспективе данный алгоритм позволит строить путь в неизвестных пространствах. 

### TurtleBot environment 

Представляет собой конфигурацию среды, в которой существует робот: пространство действий, состояния, способ сканирования препятсвий с помощью лидара и камеры, определение текущего местоположения, а также система наград и штрафов. 

Награды состоят из:
  - награда\штраф от потенциальных полей
  - награда\штраф из-за следования\отклонения от оптимального пути
  - штраф за столкновение с препяствием
  - штраф за преодоление макисмального количесвта шагов 
  - награда за достижение цели
